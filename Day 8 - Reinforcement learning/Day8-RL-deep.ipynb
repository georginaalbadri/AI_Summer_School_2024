{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the CartPole environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up DQN parameters\n",
    "state_space_size = env.observation_space.shape[0]\n",
    "action_space_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = 200\n",
    "learning_rate = 0.001\n",
    "discount_rate = 0.99\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001\n",
    "batch_size = 64\n",
    "memory_size = 10000\n",
    "target_update_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-network\n",
    "def build_model(state_space_size, action_space_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=state_space_size, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(action_space_size, activation='linear'))\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(state_space_size, action_space_size)\n",
    "target_model = build_model(state_space_size, action_space_size)\n",
    "target_model.set_weights(model.get_weights())  # Initialize target model weights\n",
    "memory = deque(maxlen=memory_size)\n",
    "rewards_all_episodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the DQN\n",
    "def train_dqn():\n",
    "    global exploration_rate\n",
    "    global target_model\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_space_size])\n",
    "        done = False\n",
    "        rewards_current_episode = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Exploration-exploitation trade-off\n",
    "            if np.random.rand() <= exploration_rate:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                q_values = model.predict(state)\n",
    "                action = np.argmax(q_values[0])\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_space_size])\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            rewards_current_episode += reward\n",
    "\n",
    "            if len(memory) > batch_size:\n",
    "                minibatch = random.sample(memory, batch_size)\n",
    "                for state_b, action_b, reward_b, next_state_b, done_b in minibatch:\n",
    "                    target = reward_b\n",
    "                    if not done_b:\n",
    "                        target += discount_rate * np.max(target_model.predict(next_state_b)[0])\n",
    "                    target_f = model.predict(state_b)\n",
    "                    target_f[0][action_b] = target\n",
    "                    model.fit(state_b, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if episode % target_update_freq == 0:\n",
    "            target_model.set_weights(model.get_weights())  # Update target model\n",
    "\n",
    "        exploration_rate = min_exploration_rate + \\\n",
    "            (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "        \n",
    "        rewards_all_episodes.append(rewards_current_episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run training\n",
    "train_dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"Average reward per thousand episodes\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(num_episodes), rewards_all_episodes)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward vs Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Q-values as a heatmap for a given state\n",
    "def plot_q_values_heatmap_for_state(model, state):\n",
    "    q_values = model.predict(state)[0]\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(np.reshape(q_values, (1, -1)), cmap='viridis', annot=True, fmt=\".1f\", cbar=True)\n",
    "    plt.title('Q-values for Given State')\n",
    "    plt.xlabel('Action')\n",
    "    plt.ylabel('Q-value')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the agent's performance\n",
    "def visualize_agent_performance(env, model):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_space_size])\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    path = []\n",
    "\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        q_values = model.predict(state)\n",
    "        action = np.argmax(q_values[0])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_space_size])\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        path.append((state, action))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Total Reward: {total_reward}\")\n",
    "\n",
    "    # Plot the path\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot([s[0][0] for s in path], label='Cart Position')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Position')\n",
    "    plt.title('Agent\\'s Cart Position Over Time')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the visualization\n",
    "visualize_agent_performance(env, model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
