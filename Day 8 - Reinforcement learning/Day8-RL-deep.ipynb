{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-learning: CartPole training\n",
    "In this environment, the goal is to keep the pole vertically by moving the cart it is in left or right.\n",
    "\n",
    "![](https://gymnasium.farama.org/_images/cart_pole.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm has two possible actions: moving left or right. It knows the angle of the pole, and the position and velocity of the cart, which it can use to train the optimal actions. These are continuous states - the position, velocity, and angle can be any real number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the CartPole environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up DQN parameters\n",
    "state_space_size = env.observation_space.shape[0]\n",
    "action_space_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_episodes = 200 # number of sequences of states, actions, and rewards\n",
    "max_steps_per_episode = 500 # max number of steps in a sequence - i.e. number of movements of the cart\n",
    "learning_rate = 0.001 # learning rate of the model (alpha)\n",
    "discount_rate = 0.99 # discount rate of the rewards (gamma)\n",
    "exploration_rate = 1 # exploration rate of the agent (epsilon)\n",
    "max_exploration_rate = 1 # max exploration rate \n",
    "min_exploration_rate = 0.01 # min exploration rate\n",
    "exploration_decay_rate = 0.001 # decay rate of the exploration rate - epsilon is reduced at each episode\n",
    "batch_size = 64 # number of samples to train the model at each step\n",
    "memory_size = 10000 # memory size of the replay buffer\n",
    "target_update_freq = 10 # frequency of updating the target Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-network architecture - 3 fully connected layers with ReLU activation\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_space_size, action_space_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_space_size, 64) # 1st hidden layer with 64 neurons\n",
    "        self.fc2 = nn.Linear(64, 64) # 2nd hidden layer with 64 neurons\n",
    "        self.fc3 = nn.Linear(64, action_space_size) # output layer with number of neurons equal to the number of actions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)) # ReLU activation function for the 1st hidden layer\n",
    "        x = torch.relu(self.fc2(x)) # ReLU activation function for the 2nd hidden layer\n",
    "        x = self.fc3(x) # output layer with no activation function (linear activation)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QNetwork(state_space_size, action_space_size)\n",
    "target_model = QNetwork(state_space_size, action_space_size)\n",
    "target_model.load_state_dict(model.state_dict())  # Initialize target model weights\n",
    "memory = deque(maxlen=memory_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "rewards_all_episodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the DQN\n",
    "def train_dqn():\n",
    "    global exploration_rate\n",
    "    global target_model\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = torch.tensor(np.reshape(state, [1, state_space_size]), dtype=torch.float32)\n",
    "        done = False\n",
    "        rewards_current_episode = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Exploration-exploitation trade-off\n",
    "            if np.random.rand() <= exploration_rate:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = model(state)\n",
    "                action = torch.argmax(q_values[0]).item()\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = torch.tensor(np.reshape(next_state, [1, state_space_size]), dtype=torch.float32)\n",
    "            reward = torch.tensor([reward], dtype=torch.float32)\n",
    "            done = torch.tensor([done], dtype=torch.float32)\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            rewards_current_episode += reward.item()\n",
    "\n",
    "            if len(memory) > batch_size:\n",
    "                minibatch = random.sample(memory, batch_size)\n",
    "                for state_b, action_b, reward_b, next_state_b, done_b in minibatch:\n",
    "                    target = reward_b\n",
    "                    if not done_b:\n",
    "                        with torch.no_grad():\n",
    "                            target += discount_rate * torch.max(target_model(next_state_b)[0])\n",
    "                    target_f = model(state_b)\n",
    "                    target_f[0][action_b] = target\n",
    "                    output = model(state_b)\n",
    "                    loss = criterion(output, target_f)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if episode % target_update_freq == 0:\n",
    "            target_model.load_state_dict(model.state_dict())  # Update target model\n",
    "\n",
    "        exploration_rate = min_exploration_rate + \\\n",
    "            (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "        \n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run training\n",
    "train_dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"Average reward per thousand episodes\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(num_episodes), rewards_all_episodes)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward vs Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Q-values as a heatmap for a given state\n",
    "def plot_q_values_heatmap_for_state(model, state):\n",
    "    with torch.no_grad():\n",
    "        q_values = model(state).numpy()[0]\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(np.reshape(q_values, (1, -1)), cmap='viridis', annot=True, fmt=\".1f\", cbar=True)\n",
    "    plt.title('Q-values for Given State')\n",
    "    plt.xlabel('Action')\n",
    "    plt.ylabel('Q-value')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the agent's performance\n",
    "def visualize_agent_performance(env, model):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(np.reshape(state, [1, state_space_size]), dtype=torch.float32)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    path = []\n",
    "\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state)\n",
    "        action = torch.argmax(q_values[0]).item()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = torch.tensor(np.reshape(next_state, [1, state_space_size]), dtype=torch.float32)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        path.append((state.numpy(), action))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Total Reward: {total_reward}\")\n",
    "\n",
    "    # Plot the path\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot([s[0][0] for s, _ in path], label='Cart Position')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Position')\n",
    "    plt.title('Agent\\'s Cart Position Over Time')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the visualization\n",
    "visualize_agent_performance(env, model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
