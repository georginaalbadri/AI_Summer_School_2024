{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-learning: CartPole training\n",
    "In this environment, the goal is to keep the pole vertically by moving the cart it is in left or right.\n",
    "\n",
    "![](https://gymnasium.farama.org/_images/cart_pole.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm has two possible actions: moving left or right. It knows the angle of the pole, and the position and velocity of the cart, which it can use to train the optimal actions. These are continuous states - the position, velocity, and angle can be any real number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://aleksandarhaber.com/wp-content/uploads/2023/01/sketch-1-1024x608.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from IPython.display import clear_output, Image, display\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the CartPole environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up DQN parameters\n",
    "state_space_size = env.observation_space.shape[0]\n",
    "action_space_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters for deep Q-learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_episodes = 200 # number of sequences of states, actions, and rewards\n",
    "max_steps_per_episode = 500 # max number of steps in a sequence - i.e. number of movements of the cart\n",
    "learning_rate = 0.001 # learning rate of the model (alpha)\n",
    "discount_rate = 0.99 # discount rate of the rewards (gamma)\n",
    "exploration_rate = 1 # exploration rate of the agent (epsilon)\n",
    "max_exploration_rate = 1 # max exploration rate \n",
    "min_exploration_rate = 0.01 # min exploration rate\n",
    "exploration_decay_rate = 0.001 # decay rate of the exploration rate - epsilon is reduced at each episode\n",
    "batch_size = 64 # number of samples to train the model at each step\n",
    "memory_size = 10000 # memory size of the replay buffer\n",
    "target_update_freq = 10 # frequency of updating the target Q-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and initialise the Q-neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-network architecture - 3 fully connected layers with ReLU activation\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_space_size, action_space_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_space_size, 24) # 1st hidden layer with 64 neurons\n",
    "        self.fc2 = nn.Linear(24, 24) # 2nd hidden layer with 64 neurons\n",
    "        self.fc3 = nn.Linear(24, action_space_size) # output layer with number of neurons equal to the number of actions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x)) # ReLU activation function for the 1st hidden layer\n",
    "        x = torch.relu(self.fc2(x)) # ReLU activation function for the 2nd hidden layer\n",
    "        x = self.fc3(x) # output layer with no activation function (linear activation)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QNetwork(state_space_size, action_space_size) # Initialize the Q-network\n",
    "target_model = QNetwork(state_space_size, action_space_size) # Initialize the target Q-network\n",
    "target_model.load_state_dict(model.state_dict())  # Initialize target model weights\n",
    "memory = deque(maxlen=memory_size) # Initialize the replay buffer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) # Initialize the optimizer\n",
    "criterion = nn.MSELoss() # Initialize the loss function\n",
    "rewards_all_episodes = [] # Initialize the list of rewards for all episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the deep Q network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the DQN\n",
    "def train_dqn():\n",
    "    global exploration_rate\n",
    "    global target_model\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = torch.tensor(np.reshape(state, [1, state_space_size]), dtype=torch.float32)\n",
    "        done = False\n",
    "        rewards_current_episode = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Exploration-exploitation trade-off\n",
    "            if np.random.rand() <= exploration_rate:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = model(state)\n",
    "                action = torch.argmax(q_values[0]).item()\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = torch.tensor(np.reshape(next_state, [1, state_space_size]), dtype=torch.float32)\n",
    "            reward = torch.tensor([reward], dtype=torch.float32)\n",
    "            done = torch.tensor([done], dtype=torch.float32)\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            rewards_current_episode += reward.item()\n",
    "\n",
    "            if len(memory) > batch_size:\n",
    "                minibatch = random.sample(memory, batch_size)\n",
    "                for state_b, action_b, reward_b, next_state_b, done_b in minibatch:\n",
    "                    target = reward_b\n",
    "                    if not done_b:\n",
    "                        with torch.no_grad():\n",
    "                            target += discount_rate * torch.max(target_model(next_state_b)[0])\n",
    "                    target_f = model(state_b)\n",
    "                    target_f[0][action_b] = target\n",
    "                    output = model(state_b)\n",
    "                    loss = criterion(output, target_f)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if episode % target_update_freq == 0:\n",
    "            target_model.load_state_dict(model.state_dict())  # Update target model\n",
    "\n",
    "        exploration_rate = min_exploration_rate + \\\n",
    "            (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "        \n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run training\n",
    "train_dqn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check training success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate and print the average reward per 10 episodes\n",
    "rewards_per_ten_episodes = np.split(np.array(rewards_all_episodes), num_episodes/10)\n",
    "count = 10\n",
    "\n",
    "print(\"Average reward per thousand episodes\")\n",
    "for r in rewards_per_ten_episodes:\n",
    "    print(count, \": \", str(sum(r/10)))\n",
    "    count += 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(num_episodes), rewards_all_episodes)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward vs Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Q-values as a heatmap for a given state\n",
    "def plot_q_values_heatmap_for_state(model, state):\n",
    "    with torch.no_grad():\n",
    "        q_values = model(state).numpy()[0]\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(np.reshape(q_values, (1, -1)), cmap='viridis', annot=True, fmt=\".1f\", cbar=True)\n",
    "    plt.title('Q-values for Given State')\n",
    "    plt.xlabel('Action')\n",
    "    plt.ylabel('Q-value')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the agent's performance\n",
    "def visualize_agent_performance(env, model):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(np.reshape(state, [1, state_space_size]), dtype=torch.float32)\n",
    "    done = False\n",
    "    frames = []\n",
    "\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        frames.append(env.render(mode=\"rgb_array\"))\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state)\n",
    "        action = torch.argmax(q_values[0]).item()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = torch.tensor(np.reshape(next_state, [1, state_space_size]), dtype=torch.float32)\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "    return frames\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create animation using Pillow\n",
    "def create_animation(frames, filename):\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.axis('off')\n",
    "    patch = plt.imshow(frames[0])\n",
    "\n",
    "    def update(frame):\n",
    "        patch.set_data(frame)\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, update, frames=frames, interval=50)\n",
    "    anim.save(filename, writer='pillow', fps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the agent's performance and create animation\n",
    "frames = visualize_agent_performance(env, model)\n",
    "create_animation(frames, 'cartpole_animation.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the saved animation\n",
    "display(Image(filename='cartpole_animation.gif'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
