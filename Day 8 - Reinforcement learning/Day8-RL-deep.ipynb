{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the CartPole environment\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up DQN parameters\n",
    "state_space_size = env.observation_space.shape[0]\n",
    "action_space_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_episodes = 1000\n",
    "max_steps_per_episode = 200\n",
    "learning_rate = 0.001\n",
    "discount_rate = 0.99\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.001\n",
    "batch_size = 64\n",
    "memory_size = 10000\n",
    "target_update_freq = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Q-network\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_space_size, action_space_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_space_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, action_space_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QNetwork(state_space_size, action_space_size)\n",
    "target_model = QNetwork(state_space_size, action_space_size)\n",
    "target_model.load_state_dict(model.state_dict())  # Initialize target model weights\n",
    "memory = deque(maxlen=memory_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "rewards_all_episodes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the DQN\n",
    "def train_dqn():\n",
    "    global exploration_rate\n",
    "    global target_model\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = torch.tensor(np.reshape(state, [1, state_space_size]), dtype=torch.float32)\n",
    "        done = False\n",
    "        rewards_current_episode = 0\n",
    "\n",
    "        for step in range(max_steps_per_episode):\n",
    "            # Exploration-exploitation trade-off\n",
    "            if np.random.rand() <= exploration_rate:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = model(state)\n",
    "                action = torch.argmax(q_values[0]).item()\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = torch.tensor(np.reshape(next_state, [1, state_space_size]), dtype=torch.float32)\n",
    "            reward = torch.tensor([reward], dtype=torch.float32)\n",
    "            done = torch.tensor([done], dtype=torch.float32)\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            rewards_current_episode += reward.item()\n",
    "\n",
    "            if len(memory) > batch_size:\n",
    "                minibatch = random.sample(memory, batch_size)\n",
    "                for state_b, action_b, reward_b, next_state_b, done_b in minibatch:\n",
    "                    target = reward_b\n",
    "                    if not done_b:\n",
    "                        with torch.no_grad():\n",
    "                            target += discount_rate * torch.max(target_model(next_state_b)[0])\n",
    "                    target_f = model(state_b)\n",
    "                    target_f[0][action_b] = target\n",
    "                    output = model(state_b)\n",
    "                    loss = criterion(output, target_f)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if episode % target_update_freq == 0:\n",
    "            target_model.load_state_dict(model.state_dict())  # Update target model\n",
    "\n",
    "        exploration_rate = min_exploration_rate + \\\n",
    "            (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "        \n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run training\n",
    "train_dqn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate and print the average reward per thousand episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"Average reward per thousand episodes\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotting the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(num_episodes), rewards_all_episodes)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward vs Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot Q-values as a heatmap for a given state\n",
    "def plot_q_values_heatmap_for_state(model, state):\n",
    "    with torch.no_grad():\n",
    "        q_values = model(state).numpy()[0]\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sns.heatmap(np.reshape(q_values, (1, -1)), cmap='viridis', annot=True, fmt=\".1f\", cbar=True)\n",
    "    plt.title('Q-values for Given State')\n",
    "    plt.xlabel('Action')\n",
    "    plt.ylabel('Q-value')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the agent's performance\n",
    "def visualize_agent_performance(env, model):\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(np.reshape(state, [1, state_space_size]), dtype=torch.float32)\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    path = []\n",
    "\n",
    "    for _ in range(max_steps_per_episode):\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state)\n",
    "        action = torch.argmax(q_values[0]).item()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = torch.tensor(np.reshape(next_state, [1, state_space_size]), dtype=torch.float32)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        path.append((state.numpy(), action))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Total Reward: {total_reward}\")\n",
    "\n",
    "    # Plot the path\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot([s[0][0] for s, _ in path], label='Cart Position')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Position')\n",
    "    plt.title('Agent\\'s Cart Position Over Time')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the visualization\n",
    "visualize_agent_performance(env, model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
