{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House price regression\n",
    "\n",
    "![houses](https://storage.googleapis.com/kaggle-media/competitions/House%20Prices/kaggle_5407_media_housesbanner.png)\n",
    "\n",
    "### Description\n",
    "\n",
    "We will now put together our knowledge of data preprocessing and linear regression to make a machine learning model for house price prediction. We will focus on feature selection and explore a couple of different linear regression model options with a goal of training the best predictive model.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Like yesterday, we will take our dataset from kaggle. This is a relatively large dataset, with as many as 79 features describing houses in Ames, Iowa. \n",
    "\n",
    "### Aims\n",
    "\n",
    "1. Explore the data to see what preprocessing is required\n",
    "2. Look into the relationships between features and the target variable to select features\n",
    "4. Train three different regression models and compare results\n",
    "5. Try again with different selected features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time we load Colab, we need to upload our kaggle.json file to access the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we need to move the kaggle.json file to the expected location  \n",
    "\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset\n",
    "Housing prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions download -c house-prices-advanced-regression-techniques\n",
    "!unzip house-prices-advanced-regression-techniques.zip\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows and basic information about the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot relationship between chosen features and target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='GrLivArea', y='SalePrice', data=df)\n",
    "plt.title('Relationship between GrLivArea (Above Grade Living Area) and Sale Price')\n",
    "plt.xlabel('GrLivArea')\n",
    "plt.ylabel('Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, plot relationship between all features and target variable, by computing the correlations between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, and to use all the features in the dataset, we need to make sure that all the features are numerical. We can do this by converting all the categorical features to numerical using one-hot encoding. We can then drop the original categorical features. We can also drop the Id column as it is not useful for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify non-numeric columns\n",
    "non_numeric_columns = df.select_dtypes(include=['object']).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert non-numeric columns using one-hot encoding\n",
    "df_encoded = pd.get_dummies(df, columns=non_numeric_columns, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the Id column\n",
    "df_encoded.drop('Id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get correlation between variables with respect to SalePrice\n",
    "correlation = df_encoded.corr()\n",
    "\n",
    "# get top 10 (positive) and bottom 10 (negative) correlation values\n",
    "top_corr = correlation['SalePrice'].sort_values(ascending=False)[:10]\n",
    "bottom_corr = correlation['SalePrice'].sort_values(ascending=True)[:10]\n",
    "\n",
    "# Concatenate top and bottom correlations\n",
    "all_corr = pd.concat([top_corr, bottom_corr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmap of correlation matrix for top 10 positively and top 10 negatively correlated features\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation.loc[all_corr.index, all_corr.index], annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap of Top 10 Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the correlation matrix, you can see which features are correlated with the SalePrice, and which features are correlated with each other to identify dependent and independent variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing = df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show only the columns that have missing values\n",
    "missing_columns = missing[missing > 0]\n",
    "\n",
    "# Convert to a percentage to see % missing values for each column\n",
    "missing_percentage = (missing_columns / len(df)) * 100\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with more than 50% missing values - or consider a lower threshold \n",
    "df_cleaned = df.dropna(thresh=0.5*len(df), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicates = df_cleaned.duplicated().sum()\n",
    "print(\"Number of duplicate rows:\", duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you think of any other checks to do on the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target variable, e.g. \n",
    "X = df_cleaned[['GrLivArea', 'YearBuilt']]\n",
    "y = df_cleaned['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables (if not already done)\n",
    "X = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into test and training sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "    print(f\"R^2 Score: {r2:.2f}\")\n",
    "    print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation and selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models visually\n",
    "plt.figure(figsize=(10, 6))\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    plt.scatter(y_test, y_pred, label=name)\n",
    "plt.xlabel('Actual Sale Price')\n",
    "plt.ylabel('Predicted Sale Price')\n",
    "plt.title('Actual vs Predicted Sale Prices')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Discuss model performance and selection criteria (e.g., MSE, R^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: optimise model hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read more about the hyperparameters of the models here: \\\n",
    "Decision trees:\\\n",
    "https://scikit-learn.org/stable/modules/tree.html \\\n",
    "Random forest:\\\n",
    "https://scikit-learn.org/stable/modules/ensemble.html#forest \\\n",
    "Finding hyperparameters and GridSearch: \\\n",
    "https://scikit-learn.org/stable/modules/grid_search.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the models again with hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hyperparameters for Decision Tree\n",
    "param_grid_dt = {\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Grid Search for Decision Tree\n",
    "grid_search_dt = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid_dt, cv=5)\n",
    "grid_search_dt.fit(X_train_scaled, y_train)\n",
    "best_dt = grid_search_dt.best_estimator_\n",
    "y_pred_dt = best_dt.predict(X_test_scaled)\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "r2_dt = r2_score(y_test, y_pred_dt)\n",
    "print(\"Decision Tree:\")\n",
    "print(f\"Best Parameters: {grid_search_dt.best_params_}\")\n",
    "print(f\"Mean Squared Error: {mse_dt:.2f}\")\n",
    "print(f\"R^2 Score: {r2_dt:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Grid Search for Random Forest\n",
    "grid_search_rf = GridSearchCV(RandomForestRegressor(random_state=42), param_grid_rf, cv=5)\n",
    "grid_search_rf.fit(X_train_scaled, y_train)\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "y_pred_rf = best_rf.predict(X_test_scaled)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest:\")\n",
    "print(f\"Best Parameters: {grid_search_rf.best_params_}\")\n",
    "print(f\"Mean Squared Error: {mse_rf:.2f}\")\n",
    "print(f\"R^2 Score: {r2_rf:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional extra: Find and implement additional regression model options on scikit-learn https://scikit-learn.org/stable/supervised_learning.html "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
